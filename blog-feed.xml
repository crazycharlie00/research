<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[kaleidoscope research]]></title><description><![CDATA[Kaleidoscope]]></description><link>https://www.kaleidoscope.glass/blog</link><generator>RSS for Node</generator><lastBuildDate>Sun, 29 Dec 2024 22:25:21 GMT</lastBuildDate><atom:link href="https://www.kaleidoscope.glass/blog-feed.xml" rel="self" type="application/rss+xml"/><item><title><![CDATA[What is a Loom?]]></title><description><![CDATA[A primer on the 'textual multiverse' and Loom interfaces.]]></description><link>https://www.kaleidoscope.glass/post/what-is-a-loom</link><guid isPermaLink="false">675cec5c3ee299bac7114f7a</guid><pubDate>Sat, 14 Dec 2024 06:57:56 GMT</pubDate><enclosure url="https://static.wixstatic.com/media/377e01_c00cb1cd06764dd3ac15a5ee9b8c3ce3~mv2.png/v1/fit/w_588,h_386,al_c,q_80/file.png" length="0" type="image/png"/><dc:creator>Chase Carter</dc:creator><content:encoded><![CDATA[<p><span style="color: #D5D4D4;"><span style="background-color: transparent;">When I say Kaleidoscope Research is “exploring the ‘textual multiverse’ implicit in large language models”, the first question is often, “</span></span><span style="color: #D5D4D4;"><span style="background-color: transparent;"><strong>...what?</strong></span></span><span style="color: #D5D4D4;"><span style="background-color: transparent;">”. In this post I’ll explain some of the relevant background, what the ‘textual multiverse’ is, and what a Loom interface is. I’ll be glossing over some of the finer technical details to keep jargon at a minimum.</span></span></p>
<p><br /></p>
<h1><span style="color: #D5D4D4;"><span style="background-color: transparent;">Background</span></span></h1>
<h2><span style="color: #D5D4D4;"><span style="background-color: transparent;">Chat Interfaces</span></span></h2>
<p><span style="color: #D5D4D4;"><span style="background-color: transparent;">You’re probably familiar with the typical chat-based interaction paradigm used for large language models (LLMs) like ChatGPT and Claude</span></span><u><span style="color: #AE5757;"><span style="background-color: transparent;"><a href="#k83st400" rel="noopener noreferrer">[1]</a></span></span></u><span style="color: #D5D4D4;"><span style="background-color: transparent;">. In this paradigm, the language model has been specifically trained to assume a helpful ‘Assistant’ personality, and to reply to the user in a conversational format: User says something, Assistant responds, and so on. If you’ve ever started a conversation with ChatGPT over from scratch, with the same initial message (and a non-zero temperature), you may have noticed that ChatGPT’s replies are non-deterministic; in other words, you can get different responses from the exact same prompt.</span></span></p>
<p><br /></p>
<p><span style="color: #D5D4D4;"><span style="background-color: transparent;">In fact, this non-determinism applies to every single word</span></span><u><span style="color: #AE5757;"><span style="background-color: transparent;"><a href="#x2g6h186" rel="noopener noreferrer">[2]</a></span></span></u><span style="color: #D5D4D4;"><span style="background-color: transparent;"> choice in the response, such that by the time you get to the end of a long response, the LLM has meandered through a potentially very path-dependent chain of word choices to arrive wherever it has arrived. Roll the dice again and you could get a very different result. Have you ever wondered about what might have been? </span></span></p>
<p><br /></p>
<p><span style="color: #D5D4D4;"><span style="background-color: transparent;">How can we answer questions like “how unlikely was this result?”, “is this result characteristic of the model’s typical behavior or is it an outlier?”, “do the possible behaviors of the model converge or diverge given such-and-such context?”, “what happens if I force the model into highly unlikely branches?”? While these questions could be explored clunkily by retrying requests over and over in a chat interface and manually copying and organizing the responses, or by setting up custom automated experiments in code, there is a better way that retains the fluidity and open-endedness of chat but also provides visibility into the diversity of possible responses.</span></span></p>
<p><br /></p>
<h2><span style="color: #D5D4D4;"><span style="background-color: transparent;">Textual Multiverse</span></span></h2>
<p><span style="color: #D5D4D4;"><span style="background-color: transparent;">With some of the predominant methods used for LLM text generation currently, for any given prompt (and non-trivial output length) there are astronomically many possible completions. There’s nothing particularly special about the one you happen to get when you press </span></span><em><span style="color: #D5D4D4;"><span style="background-color: transparent;">‘enter’ </span></span></em><span style="color: #D5D4D4;"><span style="background-color: transparent;">in a chat interface. One can imagine the other possible completions, branching out at each individual word, constituting a massive tree structure expanding outwards from the original prompt. </span></span></p>
<p><br /></p>
<p><span style="color: #D5D4D4;"><span style="background-color: transparent;">In the limit, this tree of words would contain every possible permutation of all words, which wouldn’t be a very useful data structure. But fortunately for us, LLMs, during their pre-training process, learn about the structure of language implied by human usage of language in their training dataset. Also implicit in this structure are facts or inferences (including sometimes incorrect inferences) about the context in which that language was developed and employed… which is to say, facts about the world and the humans in it. With this additional learned information, one can imagine assigning probabilities to each line between words in the tree, perhaps making more probable lines thicker and less probable lines thinner. Now we have a data structure that can actually do something useful: probabilistically generate syntactically correct &amp; semantically meaningful text based on an initial context. Or, put another way, it can simulate as much of the underlying world model as the LLM was able to learn about.</span></span><u><span style="color: #AE5757;"><span style="background-color: transparent;"><a href="#zsbm7189" rel="noopener noreferrer">[3]</a></span></span></u><span style="color: #D5D4D4;"><span style="background-color: transparent;"> This hypothetical branching data structure is the </span></span><em><span style="color: #D5D4D4;"><span style="background-color: transparent;">textual multiverse</span></span></em><span style="color: #D5D4D4;"><span style="background-color: transparent;">.</span></span><u><span style="color: #AE5757;"><span style="background-color: transparent;"><a href="#2tu33193" rel="noopener noreferrer">[4]</a></span></span></u></p>
<p><br /></p>
<p><span style="color: #D5D4D4;"><span style="background-color: transparent;">Different LLM implementations have different training processes &amp; different training datasets, and therefore different underlying world-models, so you might be wondering why we don’t say ‘textual multiverse</span></span><span style="color: #D5D4D4;"><span style="background-color: transparent;"><strong>s</strong></span></span><span style="color: #D5D4D4;"><span style="background-color: transparent;">’, plural. Technically the plural form would be accurate, however in practice it seems that as the models get larger and the datasets get bigger (approaching the total human output of recorded text), the world-models converge like different pixelated versions of the same underlying picture coming into focus.</span></span><u><span style="color: #AE5757;"><span style="background-color: transparent;"><a href="#vn1f6196" rel="noopener noreferrer">[5]</a></span></span></u><span style="color: #D5D4D4;"><span style="background-color: transparent;"> From this perspective, ‘textual multiverse’ is an abstraction over all of language, not just over any given LLM or over the collection of existing LLMs. Much like the Library of Babel, it contains every document that has ever existed or could exist.</span></span></p>
<p><br /></p>
<p><span style="color: #D5D4D4;"><span style="background-color: transparent;">It’s difficult to see the landscape of the textual multiverse in a simple chat interface; it’s like trying to infer the detailed topography of the Himalayan Mountains from a single 2D vertical cross-section. This is where Loom interfaces shine. With a Loom interface you can catch a glimpse of the shimmering landscape of concepts and intelligence embedded in language, as implied by the distribution of possible LLM completions.</span></span></p>
<p><br /></p>
<h1><span style="color: #D5D4D4;"><span style="background-color: transparent;">Loom Interfaces</span></span></h1>
<p><span style="color: #D5D4D4;">“(The) Loom, short for the Loom of Time, is an interface to probabilistic generative models which allows users to efficiently generate, navigate, save, and filter multiverses. ‘Loom’ may refer to any of the existing software implementations as well as hypothetical interfaces sharing the schema. Loom [...] is especially suited to base models. The act of using Loom is sometimes called ‘looming’ or ‘weaving’.”</span></p>
<p style="text-align: right;"><span style="color: #D5D4D4;"><span style="background-color: transparent;">– “Loom”, Cyborgism Wiki</span></span><u><span style="color: #AE5757;"><span style="background-color: transparent;"><a href="#uf660199" rel="noopener noreferrer">[6]</a></span></span></u></p>
<p><br /></p>
<p><span style="color: #D5D4D4;"><span style="background-color: transparent;">The central feature of a Loom interface is the ability to generate multiple completions for one prompt, select one of the completions, and repeat. Each completion becomes a node in a large tree-shaped graph, where the full prompt for its generation is the combined contents of all of its parent nodes all the way back to the root node (which contains the original user prompt). Note that this means that the user is not necessarily interacting with the LLM as an ‘Assistant’ entity anymore; more likely, the user will be interacting with the system as a highly advanced autocomplete generating a document such as a work of fiction, a blog post, a movie script, a news article, a computer console session, a chat log, a sacred text, a secret message, etc. </span></span></p>
<p><br /></p>
<figure><img src="https://static.wixstatic.com/media/377e01_c07b541ee27a4145a7d0397e8842faf2~mv2.png/v1/fit/w_1000,h_940,al_c,q_80/file.png"title="Exoloom UI (in development)"></figure>
<p><span style="color: #D5D4D4;"><span style="background-color: transparent;">Usually a graphical representation of the tree structure thus generated is presented to facilitate navigation. Often, the user is also allowed to add their own input at arbitrary nodes, driving the outcome in a desired direction. Alternatively, the user may forgo adding their own input, trying to see where they can get solely through curation of completions.</span></span></p>
<p><br /></p>
<p><span style="color: #D5D4D4;"><span style="background-color: transparent;">The Loom interaction paradigm was initially explored by Love Laverkvist in their &quot;</span></span><u><a href="https://motform.org/multiverse" target="_blank"><span style="color: #AE5757;"><span style="background-color: transparent;">Multiverse</span></span></a></u><span style="color: #D5D4D4;"><span style="background-color: transparent;">&quot; app, and Janus in their original &quot;</span></span><u><a href="https://generative.ink/posts/loom-interface-to-the-multiverse/" target="_blank"><span style="color: #AE5757;"><span style="background-color: transparent;">Loom</span></span></a></u><span style="color: #D5D4D4;"><span style="background-color: transparent;">&quot; app. </span></span></p>
<p><br /></p>
<p><span style="color: #D5D4D4;"><span style="background-color: transparent;">Loom-style interfaces open up </span></span><em><span style="color: #D5D4D4;"><span style="background-color: transparent;">many </span></span></em><span style="color: #D5D4D4;"><span style="background-color: transparent;">possibilities, but some of the most obvious are:​</span></span></p>
<ul>
  <li><p><span style="color: #D5D4D4;"><span style="background-color: transparent;">The user can see &apos;what might have been&apos;, get a broader sense of what the LLM is capable of, and explore the &apos;textual multiverse&apos; implicit in LLMs.​</span></span></p></li>
  <li><p><span style="color: #D5D4D4;"><span style="background-color: transparent;">The user can exert much more control over the interaction by curating completions (choosing which branches to proceed along) and interfacing at a finer level. In the limit, boundaries blur and the user and the LLM become a single symbiotic agent, a &quot;</span></span><u><a href="https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism" target="_blank"><span style="color: #AE5757;"><span style="background-color: transparent;">Cyborg</span></span></a></u><span style="color: #D5D4D4;"><span style="background-color: transparent;">&quot;.</span></span></p></li>
  <li><p><span style="color: #D5D4D4;"><span style="background-color: transparent;">They are an ideal interface for creating and consuming non-linear ‘multiversal fiction’, which at its simplest can be like a choose-your-own-adventure novel.</span></span></p></li>
  <li><p><span style="color: #D5D4D4;"><span style="background-color: transparent;">They provide a workable and flexible way to interact with base models (which have not been trained to interact in a chat paradigm or have an ‘Assistant’ personality, and require curation in order maintain coherence).</span></span></p><p><br /></p></li>
</ul>
<h2><span style="color: #D5D4D4;"><span style="background-color: transparent;">Exoloom</span></span></h2>
<p><span style="color: #D5D4D4;"><span style="background-color: transparent;">With our Exoloom project, we’re hoping to make the textual multiverse more accessible to a broader range of researchers and AI enthusiasts. We’ll be hosting a full-featured Loom implementation on the web alongside a hub for sharing results and collaborating on shared Loom trees in realtime. From there, we’ll launch expeditions into this vast textual space that is perpetually unfolding, mapping its basins and discovering its denizens, hopefully gaining valuable insight into what we can expect in the rapidly approaching future when LLMs will affect us and our context as much as we affect them and theirs. </span></span></p>
<p><br /></p>
<p><br /></p>
<hr>
<p><span style="color: rgb(213, 212, 212);">Notes:</span></p>
<p><u><span style="color: #AE5757;"><a href="#6m8za307" rel="noopener noreferrer">[1]</a></span></u><span style="color: #D5D4D4;"> and if not, what are you doing? It’s 2024. </span><u><a href="https://www.google.com/url?q=https://www.anthropic.com/claude&sa=D&source=editors&ust=1734149274673860&usg=AOvVaw313T-q0PzqeHQjEocO4VuA" target="_blank"><span style="color: #AE5757;">Go talk to Claude</span></a></u><span style="color: #D5D4D4;">. </span></p>
<p><u><span style="color: #AE5757;"><a href="#cinye310" rel="noopener noreferrer">[2]</a></span></u><span style="color: #D5D4D4;"> technically ‘token’ rather than ‘word’, but the two are similar enough that I’ll use ‘word’ throughout this post to minimize jargon.</span></p>
<p><u><span style="color: #AE5757;"><a href="#k7dad322" rel="noopener noreferrer">[3]</a></span></u><span style="color: #D5D4D4;"> see: </span><u><a href="https://www.google.com/url?q=https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators&sa=D&source=editors&ust=1734149274674053&usg=AOvVaw3KPyX1i6n_GJAKVuZiTSVI" target="_blank"><span style="color: #AE5757;">Simulators — LessWrong</span></a></u></p>
<p><u><span style="color: #AE5757;"><a href="#k7dad322" rel="noopener noreferrer">[4]</a></span></u><span style="color: #D5D4D4;"> note: the textual multiverse is a hypothetical/conceptual structure, not a description of an actual data structure or process used by LLMs to generate outputs. While ‘assigning probabilities for the next word’ is part of the LLM inference process, the static tree-like structure of words with baked-in probabilities is not a part of LLM inference.</span></p>
<p><u><span style="color: #AE5757;"><a href="#k8zmk327" rel="noopener noreferrer">[5]</a></span></u><span style="color: #D5D4D4;"> The picture evolves over time as underlying conditions evolve and the usage of language feeds into the further development of language; in this sense it’s more like a video than a picture. We expect this to accelerate as LLMs’ and AI agents’ presence in the world and usage of language strengthens this feedback loop. But that’s a topic for another post.</span></p>
<p><u><span style="color: #AE5757;"><a href="#tw51o338" rel="noopener noreferrer">[6]</a></span></u><span style="color: #D5D4D4;"> </span><u><a href="https://cyborgism.wiki/hypha/loom" target="_blank"><span style="color: #AE5757;">https://cyborgism.wiki/hypha/loom</span></a></u><span style="color: #D5D4D4;"> </span></p>
<p><br /></p>]]></content:encoded></item></channel></rss>